We will first learn about q-learning
Section 1   Lecture 6
- What is reinforcemnet learning
- The bellman equation
- Navigate inside Ai
- Markov Decision Proceed
- Policy vs plan
- Adding a living penalitlu
- Q-learing
- temporal difference
- Visualisation


Lecture 7     Reinforcement Learing
- Environment 
-Agent
Agent performs the actions in the environment and will get a reward for the change of the state of the agent. Any action of our day to day life have routines for we assign that routine to this agent to perform the action in the particular environment.Day to day examples Driving a car, and making an omellete, trading in the stock market, .Training an Ai is like training a Dog like on a data sets .
Robot Dogs   -> Tells us the difference between preprogrammed and reinforcement
Preprogrammed to walk there is an fixed algorithm like an function, whereas in reinforcemnt there is no algorithm instead it has a reinforcent learing algorithm that defines certain function and then it leaves the dog to figure out how to walk, it learns to walk


Lecture  8     Bellman Equation
Concepts ->
- s- state
-  a- Action
- R -reward
- Y(gamma) - Discount
V(s){Weight of each block in this example}
R(s,a){Reward of the agent}
V(s'){value or weight of the new state}
Y{because to decide which way to go}

V(s)=max(R(s,a)+YV(s'))
	a


Lecture 9    The PLan
  we have a maze and the value of each cells 


Lecture 10    MDPs
Deterministic Search
If the agent decideds to go up then 100% probability it will go up once pressed the key

NonDetermininstic Search
There are couple of options with divided probability 

So the aim is to make a more real world approach then in order for the Ai to perform fine then we may prefer NDS

Markov Property
is wen ur future states is result on where u r now and not depend on how u got there what will happen in future only depends on what u do next and randomness of programm


MDP

Partly random and partly under control
add on to the bellman eq

here the YV(s') gets divided into the no of choices you can 
take from that particular position multiplied with the
 respeted properties of reaching there i.e  YV(s')=p1*V(s1')+p2*V(s2')+p3V(s3')


Lecture 12 living penalty

living penalty is like a negative reward

lecture 13 
Q learing





lecture 17























